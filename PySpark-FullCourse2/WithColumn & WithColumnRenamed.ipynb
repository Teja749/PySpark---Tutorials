{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8567a28f-a9ed-4f5d-9019-a7d8cffa3c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " /FileStore/tables/Emp_1-1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efe3e8a5-7e88-42c8-93a9-aca7a3c1bf95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**withColumn() function is used to add a new column, replace an existing column, or transform an existing column in a DataFrame.\n",
    "\n",
    "In PySpark, you can use the withColumnRenamed() method to rename a column in a DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2ed9ed-44da-4721-b332-7f6209d2d07c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read a DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/FileStore/tables/Emp_1-1.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca988e0-facb-46c0-83fb-ae274bc63504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+----+-----+\n|emp_id|  name|salary|address| loc|email|\n+------+------+------+-------+----+-----+\n|     1|manish| 10000|  india|null| null|\n|     2|  rani| 20000|    usa|null| null|\n|     3| rinku| 55000|  india|null| null|\n|     4|  neha| 12000|    usa|null| null|\n|     5|  null| 20000|    usa|null| null|\n|     6| rahul|  null|  india|null| null|\n|     1|manish| 10000|  india|null| null|\n|     2|  rani| 20000|    usa|null| null|\n|     3| rinku| 55000|  india|null| null|\n|     4|  neha| 12000|    usa|null| null|\n|     5|  null| 20000|    usa|null| null|\n|     6| rahul|  null|  india|null| null|\n+------+------+------+-------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74d2c11-c91d-41b3-a96c-2080ac4cbcfb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add New Column Based on existing Column"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+----+-----+--------+\n|emp_id|  name|salary|address| loc|email| salary1|\n+------+------+------+-------+----+-----+--------+\n|     1|manish| 10000|  india|null| null|100000.0|\n|     2|  rani| 20000|    usa|null| null|200000.0|\n|     3| rinku| 55000|  india|null| null|550000.0|\n|     4|  neha| 12000|    usa|null| null|120000.0|\n|     5|  null| 20000|    usa|null| null|200000.0|\n|     6| rahul|  null|  india|null| null|    null|\n|     1|manish| 10000|  india|null| null|100000.0|\n|     2|  rani| 20000|    usa|null| null|200000.0|\n|     3| rinku| 55000|  india|null| null|550000.0|\n|     4|  neha| 12000|    usa|null| null|120000.0|\n|     5|  null| 20000|    usa|null| null|200000.0|\n|     6| rahul|  null|  india|null| null|    null|\n+------+------+------+-------+----+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary1\",df.salary*10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7334f2b-9b49-4050-b806-255941a00899",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add New Column Based on Constant Value"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+----+-----+-------+\n|emp_id|  name|salary|address| loc|email|Country|\n+------+------+------+-------+----+-----+-------+\n|     1|manish| 10000|  india|null| null|    USA|\n|     2|  rani| 20000|    usa|null| null|    USA|\n|     3| rinku| 55000|  india|null| null|    USA|\n|     4|  neha| 12000|    usa|null| null|    USA|\n|     5|  null| 20000|    usa|null| null|    USA|\n|     6| rahul|  null|  india|null| null|    USA|\n|     1|manish| 10000|  india|null| null|    USA|\n|     2|  rani| 20000|    usa|null| null|    USA|\n|     3| rinku| 55000|  india|null| null|    USA|\n|     4|  neha| 12000|    usa|null| null|    USA|\n|     5|  null| 20000|    usa|null| null|    USA|\n|     6| rahul|  null|  india|null| null|    USA|\n+------+------+------+-------+----+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"Country\",lit(\"USA\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078c64ef-42f7-4dcd-8643-bcd47bc3420e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+----+-----+---------+\n|emp_id|  name|salary|address| loc|email|increment|\n+------+------+------+-------+----+-----+---------+\n|     1|manish| 10000|  india|null| null|        0|\n|     2|  rani| 20000|    usa|null| null|        0|\n|     3| rinku| 55000|  india|null| null|        0|\n|     4|  neha| 12000|    usa|null| null|        0|\n|     5|  null| 20000|    usa|null| null|        0|\n|     6| rahul|  null|  india|null| null|        0|\n|     1|manish| 10000|  india|null| null|        0|\n|     2|  rani| 20000|    usa|null| null|        0|\n|     3| rinku| 55000|  india|null| null|        0|\n|     4|  neha| 12000|    usa|null| null|        0|\n|     5|  null| 20000|    usa|null| null|        0|\n|     6| rahul|  null|  india|null| null|        0|\n+------+------+------+-------+----+-----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"increment\",lit(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b343532-6ba6-4d5e-92b9-08de53ced660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Update Existing Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a783d3-d517-4a8e-a802-ce7cc13706b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+----+-----+\n|emp_id|  name|salary|address| loc|email|\n+------+------+------+-------+----+-----+\n|     1|manish| 10000|  india|null| null|\n|     2|  rani| 20000|    usa|null| null|\n|     3| rinku| 55000|  india|null| null|\n|     4|  neha| 12000|    usa|null| null|\n|     5|  null| 20000|    usa|null| null|\n|     6| rahul|  null|  india|null| null|\n|     1|manish| 10000|  india|null| null|\n|     2|  rani| 20000|    usa|null| null|\n|     3| rinku| 55000|  india|null| null|\n|     4|  neha| 12000|    usa|null| null|\n|     5|  null| 20000|    usa|null| null|\n|     6| rahul|  null|  india|null| null|\n+------+------+------+-------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746d3c5b-491e-4493-8179-980493adf65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- address: string (nullable = true)\n |-- loc: string (nullable = true)\n |-- email: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5124226-ec7e-4a39-9732-d467eca678ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import col\n",
    "df1 = df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aaba142-2a37-4d2c-80ea-61d16857bcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+----+-----+\n|emp_id|  name|salary|address| loc|email|\n+------+------+------+-------+----+-----+\n|     1|manish| 10000|  india|null| null|\n|     2|  rani| 20000|    usa|null| null|\n|     3| rinku| 55000|  india|null| null|\n|     4|  neha| 12000|    usa|null| null|\n|     5|  null| 20000|    usa|null| null|\n|     6| rahul|  null|  india|null| null|\n|     1|manish| 10000|  india|null| null|\n|     2|  rani| 20000|    usa|null| null|\n|     3| rinku| 55000|  india|null| null|\n|     4|  neha| 12000|    usa|null| null|\n|     5|  null| 20000|    usa|null| null|\n|     6| rahul|  null|  india|null| null|\n+------+------+------+-------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03be3c17-2e17-4223-8788-650893661252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- address: string (nullable = true)\n |-- loc: string (nullable = true)\n |-- email: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1539ecfe-362a-4347-9d00-bf4efc96a376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Rename the Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c3acc8-c024-4aa0-847e-d368eba7f3eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+--------+-----+\n|emp_id|  name|salary|address|location|email|\n+------+------+------+-------+--------+-----+\n|     1|manish| 10000|  india|    null| null|\n|     2|  rani| 20000|    usa|    null| null|\n|     3| rinku| 55000|  india|    null| null|\n|     4|  neha| 12000|    usa|    null| null|\n|     5|  null| 20000|    usa|    null| null|\n|     6| rahul|  null|  india|    null| null|\n|     1|manish| 10000|  india|    null| null|\n|     2|  rani| 20000|    usa|    null| null|\n|     3| rinku| 55000|  india|    null| null|\n|     4|  neha| 12000|    usa|    null| null|\n|     5|  null| 20000|    usa|    null| null|\n|     6| rahul|  null|  india|    null| null|\n+------+------+------+-------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"loc\",\"location\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "WithColumn & WithColumnRenamed",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
